<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
	    <!-- Global site tag (gtag.js) - Google Analytics -->
	    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BL3NBNBTY3"></script>
	    <script>
	      window.dataLayer = window.dataLayer || [];
	      function gtag(){dataLayer.push(arguments);}
	      gtag('js', new Date());
	      gtag('config', 'G-BL3NBNBTY3');
	    </script>


	  <style type="text/css">
	  	.center {
		  display: block;
		  margin-left: auto;
		  margin-right: auto;
		  width: 50%;
		}
	  </style>

	  <link rel="shortcut icon" href="https://www.smu.edu.sg/sites/all/themes/smu/images/smu_favicon.png" type="image/png" />

	<title>Deeplight : Robust & Unobtrusive Real-time Screen-Camera Communication for Real-World Displays
</title>
</head>

<body>
<h3>Deeplight : Robust & Unobtrusive Real-time Screen-Camera Communication for Real-World Displays</h3>
<hr>

<a href="http://www.cs.ox.ac.uk/people/vu.tran/">Vu Tran</a>, <a href="https://gihan.me">Gihan Jayatilaka</a>, <a href="https://mobile.cs.gsu.edu/aashok/">Ashwin Ashok</a> and <a href="https://sites.google.com/view/archan-misra">Archan Misra</a>, 2021, April. <b>Deeplight : Robust & Unobtrusive Real-time Screen-Camera Communication for Real-World Displays.</b> In 2021 20th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) (). IEEE.<br>
</p>

[<a href="https://arxiv.org/abs/2105.05092">Preprint PDF</a>]

<h4>Download</h4>
<ul>
	<li><a href="https://github.com/LARC-CMU-SMU/deeplight">Code</a></li>
	<li><a href="https://tesla.ce.pdn.ac.lk/gihan-deeplight/">ScreenNet weights</a></li>
	<li><a href="#">LightNet weights</a></li>
	<li><a href="https://tesla.ce.pdn.ac.lk/gihan-deeplight/screen-net-dataset/">Dataset for ScreenNet</a> (Only a part of dataset is here)</li>
</ul>

<h4>Overview diagram</h4>
<img src="https://github.com/LARC-CMU-SMU/deeplight/raw/master/documentation/overview.png" class="center">

<h4>System architecture</h4>
<img src="https://github.com/LARC-CMU-SMU/deeplight/raw/master/documentation/system-architecture.png" class="center">

<h4>Cite</h4>

<pre class="center"><code>@InProceedings{deeplight2021,
	author = {Tran, Vu and Jayatilaka, Gihan and Ashok, Ashwin and Misra, Archan},
	title = {Deeplight : Robust &amp; Unobtrusive Real-time Screen-Camera Communication for Real-World Displays},
	booktitle = {ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
	month = {May},
	year = {2021}
}
</code></pre>

<h4>Supplementary material</h4>
  1. The images used to train DeepLight Screen Extractor [<a href="https://github.com/LARC-CMU-SMU/deeplight/tree/master/docs/screen-detection-indoor">Indoor scenes</a>, <a href="https://github.com/LARC-CMU-SMU/deeplight/tree/master/docs/screen-detection-outdoor">Outdoor scenes</a>]<br>
  2. The representative <a href="https://github.com/LARC-CMU-SMU/deeplight/tree/master/docs/user_study_videos">images</a> of videos used in the user study <br>
  3. The representative <a href="https://github.com/LARC-CMU-SMU/deeplight/tree/master/docs/lightnet-train">images</a> of videos used to train DeepLight Decoder <br>
  4. The representative <a href="https://github.com/LARC-CMU-SMU/deeplight/tree/master/docs/experiment_videos">images of videos</a> used in the experiment <br>
  5. A video showing DeepLight operation in real-time (<a href="https://github.com/deeplightscc/deeplightscc.github.io/blob/master/deeplight_demo.mov">deeplight_demo.mov</a>).<br> 
  In this demo, the iPhone captures video frames of a transmitter screen and process them locally. After the user presses the start button, the DeepLight pipe-line runs for 32 frames and continuously update the results while processing the frames. The screen detector runs only once (the first frame) for 32 frames. The reponse time of DeepLight shows that it supports real-time processing.<br>
</body>
</html>
