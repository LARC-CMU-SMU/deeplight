<html>
<title>DeepLight: ML-Driven Robust Screen-Camera Communication for Public Displays</title>
<body>
<head>DeepLight: ML-Driven Robust Screen-Camera Communication for Public Displays</head>
  Some materials for DeepLight <br>
  1. The images used to train DeepLight Screen Extractor [<a href="https://github.com/deeplightscc/deeplightscc.github.io/tree/master/screen-detection-indoor">Indoor scenes</a>, <a href="https://github.com/deeplightscc/deeplightscc.github.io/tree/master/screen-detection-outdoor">Outdoor scenes</a>]<br>
  2. The representative <a href="https://github.com/deeplightscc/deeplightscc.github.io/tree/master/user_study_videos">images</a> of videos used in the user study <br>
  3. The representative <a href="https://github.com/deeplightscc/deeplightscc.github.io/tree/master/lightnet-train">images</a> of videos used to train DeepLight Decoder <br>
  4. The representative <a href="https://github.com/deeplightscc/deeplightscc.github.io/tree/master/experiment_videos">images of videos</a> used in the experiment <br>
  5. A video showing DeepLight operation in real-time (<a href="https://github.com/deeplightscc/deeplightscc.github.io/blob/master/deeplight_demo.mov">deeplight_demo.mov</a>).<br> 
  In this demo, the iPhone captures video frames of a transmitter screen and process them locally. After the user presses the start button, the DeepLight pipe-line runs for 32 frames and continuously update the results while processing the frames. The screen detector runs only once (the first frame) for 32 frames. The reponse time of DeepLight shows that it supports real-time processing.<br>
</body>
</html>
